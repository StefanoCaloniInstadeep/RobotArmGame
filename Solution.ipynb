{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Navigation\n",
    "\n",
    "---\n",
    "\n",
    "In this notebook, you will learn how to use the Unity ML-Agents environment for the second project of the [Deep Reinforcement Learning Nanodegree](https://www.udacity.com/course/deep-reinforcement-learning-nanodegree--nd893).\n",
    "\n",
    "### 1. Start the Environment\n",
    "\n",
    "We begin by importing some necessary packages.  If the code cell below returns an error, please revisit the project instructions to double-check that you have installed [Unity ML-Agents](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Installation.md) and [NumPy](http://www.numpy.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will start the environment!  **_Before running the code cell below_**, change the `file_name` parameter to match the location of the Unity environment that you downloaded.\n",
    "\n",
    "- **Mac**: `\"path/to/Reacher.app\"`\n",
    "- **Windows** (x86): `\"path/to/Reacher_Windows_x86/Reacher.exe\"`\n",
    "- **Windows** (x86_64): `\"path/to/Reacher_Windows_x86_64/Reacher.exe\"`\n",
    "- **Linux** (x86): `\"path/to/Reacher_Linux/Reacher.x86\"`\n",
    "- **Linux** (x86_64): `\"path/to/Reacher_Linux/Reacher.x86_64\"`\n",
    "- **Linux** (x86, headless): `\"path/to/Reacher_Linux_NoVis/Reacher.x86\"`\n",
    "- **Linux** (x86_64, headless): `\"path/to/Reacher_Linux_NoVis/Reacher.x86_64\"`\n",
    "\n",
    "In the repository, it is already provided the linux version (with 1 or 20 agents):\n",
    "```\n",
    "env = UnityEnvironment(file_name=\"Reacher_Linux/Reacher.x86_64\")\n",
    "env = UnityEnvironment(file_name=\"Reacher_Linux_20/Reacher.x86_64\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\tgoal_speed -> 1.0\n",
      "\t\tgoal_size -> 5.0\n",
      "Unity brain name: ReacherBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 33\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    }
   ],
   "source": [
    "env = UnityEnvironment(file_name=\"Reacher_Linux_20/Reacher.x86_64\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environments contain **_brains_** which are responsible for deciding the actions of their associated agents. Here we check for the first brain available, and set it as the default brain we will be controlling from Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Examine the State and Action Spaces\n",
    "\n",
    "The simulation contains 20 agents.  At each time step, it has four continuous actions to pick within -1 and +1.\n",
    "\n",
    "The state space has `33` dimensions.  A reward is given whenever a step is inside the target ball. \n",
    "\n",
    "Run the code cell below to print some information about the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agents count: 20\n",
      "State size: 33\n",
      "Action size: 4\n"
     ]
    }
   ],
   "source": [
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "NUM_AGENTS = len(env_info.agents)\n",
    "STATE_SIZE = env_info.vector_observations.shape[1]\n",
    "ACTION_SIZE = brain.vector_action_space_size\n",
    "\n",
    "print('Agents count:', NUM_AGENTS)\n",
    "print('State size:', STATE_SIZE)\n",
    "print('Action size:', ACTION_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Take Random Actions in the Environment\n",
    "\n",
    "In the next code cell, you will learn how to use the Python API to control the agent and receive feedback from the environment.\n",
    "\n",
    "Once this cell is executed, you will watch the agent's performance, if it selects an action (uniformly) at random with each time step.  A window should pop up that allows you to observe the agent, as it moves through the environment.  \n",
    "\n",
    "Of course, as part of the project, you'll have to change the code so that the agent is able to use its experience to gradually choose better actions when interacting with the environment!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.16399999633431434\n"
     ]
    }
   ],
   "source": [
    "env_info = env.reset(train_mode=True)[brain_name]  # reset the environment (set train_mode=False to be slow)\n",
    "state = env_info.vector_observations[0]            # get the current state\n",
    "scores = np.zeros(NUM_AGENTS)                       # initialize the score\n",
    "dones = np.zeros(NUM_AGENTS)                        # initialize the done\n",
    "while not np.any(dones):\n",
    "    actions = np.random.randn(NUM_AGENTS, ACTION_SIZE)        \n",
    "    actions = np.clip(actions, -1,1)               # random actions but clipped to -1,1\n",
    "    env_info = env.step(actions)[brain_name]       # send the action to the environment\n",
    "    rewards = env_info.rewards                      # get the reward\n",
    "    dones = env_info.local_done                  # see if episode has finished\n",
    "    scores += rewards                                # update the score\n",
    "\n",
    "total_score = np.mean(scores) \n",
    "print(\"Score: {}\".format(total_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. It's Your Turn!\n",
    "\n",
    "Now it's your turn to train your own agent to solve the environment!  When training the environment, set `train_mode=True`, so that the line for resetting the environment looks like the following:\n",
    "```python\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "```\n",
    "\n",
    "\n",
    "Let's set up some parameters, feel free to change them.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import torch.nn as nn\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "SEED = 0\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "# the hyperparameters\n",
    "GAMMA = 0.99\n",
    "TAU = 0.95\n",
    "LR = 2e-4\n",
    "ENTROPY_BETA = 0.001\n",
    "NUM_STEPS = 512\n",
    "BATCH_SIZE = 128\n",
    "PPO_UPDATES = 4\n",
    "EPS = 0.2\n",
    "\n",
    "# the model\n",
    "FC1 = 64\n",
    "FC2 = 64\n",
    "LOG_STD = 0.0\n",
    "\n",
    "ACTOR_FUNCTION = nn.Tanh()  # function to use in actor network\n",
    "\n",
    "TARGET = 30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time to create our agent.\n",
    "\n",
    "It is a basic actor critic network.\n",
    "- The critic is for estimating the value given a state.\n",
    "- The actor is for choosing the action given a state.\n",
    "\n",
    "Note that the `ACTOR_FUNCTION` is `nn.Tanh` that will leads to -1,1.\n",
    "That's exactly what our environment is expecting. No need to rescale it!\n",
    "\n",
    "However this is not enough, the network gives us back a mean value. We sample the action using a gaussian distribution `LOG_STD`. \n",
    "\n",
    "Don't worry, to be safe we will clip the output before giving it to the step function of the environment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.distributions import Normal\n",
    "\n",
    "class Agent(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Agent, self).__init__()\n",
    "        self.critic = nn.Sequential(\n",
    "            nn.Linear(STATE_SIZE, FC1),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(FC1, FC2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(FC2, 1),\n",
    "        )\n",
    "        self.actor = nn.Sequential(\n",
    "            nn.Linear(STATE_SIZE, FC1),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(FC1, FC2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(FC2, ACTION_SIZE),\n",
    "            ACTOR_FUNCTION\n",
    "        )\n",
    "\n",
    "        self.log_std = nn.Parameter(torch.ones(1, ACTION_SIZE) * LOG_STD)\n",
    "\n",
    "    def value(self, state):\n",
    "        return self.critic(state)\n",
    "\n",
    "    def dist(self, state):\n",
    "        action_mean = self.actor(state)\n",
    "        std = self.log_std.exp().expand_as(action_mean)\n",
    "        return Normal(action_mean, std)\n",
    "\n",
    "    def act(self, state):\n",
    "        dist = self.dist(state)\n",
    "        value = self.value(state)\n",
    "\n",
    "        action = dist.sample()\n",
    "        log_prob = dist.log_prob(action)\n",
    "        entropy = dist.entropy().mean()\n",
    "\n",
    "        return action, log_prob, entropy, value\n",
    "    \n",
    "    def save(self, path):\n",
    "        torch.save(self.state_dict(), path)\n",
    "        \n",
    "    def load(self, path):\n",
    "        self.load_state_dict(torch.load(path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's write a small helper function to evaluate our agent.\n",
    "We will use it also during training to see how the agent is doing and if it is able to reach our `TARGET`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.12649999717250465\n"
     ]
    }
   ],
   "source": [
    "def evaluate_agent(agent, fast=True):\n",
    "    env_info = env.reset(train_mode=fast)[brain_name]\n",
    "    states = torch.Tensor(env_info.vector_observations).to(device)\n",
    "\n",
    "    scores = np.zeros(NUM_AGENTS)\n",
    "    dones = np.zeros(NUM_AGENTS)\n",
    "    while not np.any(dones):\n",
    "        actions, _, _, _ = agent.act(states)\n",
    "        env_info = env.step(np.clip(actions.cpu().numpy(), -1, 1))[brain_name]\n",
    "        states = torch.Tensor(env_info.vector_observations).to(device)\n",
    "        rewards = env_info.rewards\n",
    "        dones = env_info.local_done\n",
    "        scores += rewards        \n",
    "\n",
    "    total_score = np.mean(scores)\n",
    "    return total_score\n",
    "\n",
    "agent = Agent()\n",
    "\n",
    "score = evaluate_agent(agent)\n",
    "print(\"Score: {}\".format(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not any better than the random agent.\n",
    "\n",
    "Time to train it!\n",
    "\n",
    "A couple of helper function first:\n",
    "- run_rollout(agent) performs some steps (`NUM_STEPS`) and save it it a memory. This is the data we use for training\n",
    "- compute_returns(..) computes the expected returns based on "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_returns(final_value, rewards, multipliers, values):\n",
    "    gae = 0\n",
    "\n",
    "    values = values.reshape((NUM_STEPS, NUM_AGENTS, 1)).to(device).detach()\n",
    "    next_value = final_value.reshape((NUM_AGENTS, 1)).to(device).detach()\n",
    "    returns = torch.zeros((NUM_STEPS, NUM_AGENTS, 1)).to(device).detach()\n",
    "    for step in reversed(range(NUM_STEPS)):\n",
    "        delta = rewards[step] + GAMMA * next_value * \\\n",
    "            multipliers[step] - values[step]\n",
    "        gae = delta + GAMMA * TAU * multipliers[step] * gae\n",
    "        returns[step] = gae + values[step]\n",
    "        next_value = values[step]\n",
    "    return returns\n",
    "\n",
    "\n",
    "def run_rollout(agent):\n",
    "    states = torch.zeros((NUM_STEPS, NUM_AGENTS, STATE_SIZE)).to(device)\n",
    "    actions = torch.zeros((NUM_STEPS, NUM_AGENTS, ACTION_SIZE)).to(device)\n",
    "    log_probs = torch.zeros((NUM_STEPS, NUM_AGENTS, ACTION_SIZE)).to(device)\n",
    "    rewards = torch.zeros((NUM_STEPS, NUM_AGENTS, 1)).to(device)\n",
    "    multipliers = torch.zeros((NUM_STEPS, NUM_AGENTS, 1)).to(device)\n",
    "    values = torch.zeros((NUM_STEPS, NUM_AGENTS, 1)).to(device)\n",
    "\n",
    "    env_info = env.reset(train_mode=True)[brain_name]\n",
    "    state = torch.Tensor(env_info.vector_observations).to(device)\n",
    "    for t in range(NUM_STEPS):\n",
    "        action, log_prob, _, value = agent.act(state)\n",
    "        env_info = env.step(np.clip(action.cpu().numpy(), -1, 1))[brain_name]\n",
    "\n",
    "        next_state = torch.Tensor(env_info.vector_observations).to(device)\n",
    "        reward = torch.tensor(env_info.rewards).unsqueeze(1).detach()\n",
    "        done = torch.tensor(1-np.asarray(env_info.local_done)\n",
    "                            ).unsqueeze(1).detach()\n",
    "\n",
    "        states[t] = state.detach()\n",
    "        actions[t] = action.detach()\n",
    "        log_probs[t] = log_prob.detach()\n",
    "        rewards[t] = reward\n",
    "        multipliers[t] = done\n",
    "        values[t] = value.detach()\n",
    "\n",
    "        state = next_state\n",
    "\n",
    "    _, _, _, next_value = agent.act(next_state)\n",
    "    returns = compute_returns(next_value, rewards, multipliers, values)\n",
    "\n",
    "    returns = returns.reshape((NUM_STEPS*NUM_AGENTS, 1)).detach()\n",
    "    log_probs = log_probs.reshape((-1, ACTION_SIZE)).detach()\n",
    "    values = values.reshape((-1, 1)).detach()\n",
    "    states = states.reshape((-1, STATE_SIZE)).detach()\n",
    "    actions = actions.reshape((-1, ACTION_SIZE)).detach()\n",
    "    advantages = (returns - values).to(device).detach()\n",
    "    \n",
    "    return states, actions, log_probs, returns, advantages        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can finally train the agent and save the weights.\n",
    "Iteratively:\n",
    "- we create a rollout, this is the memory we train on\n",
    "- we then optimize the weights, we can perform PPO_UPDATES before discarding the memory. Remember the PPO is clipping the surrogate function so the new policy will be close enough to the one used for creating the rollout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0 Score: 0.0824999981559813 Mean window score: 0.0824999981559813\n",
      "Episode: 1 Score: 0.21399999521672725 Mean window score: 0.14824999668635427\n",
      "Episode: 2 Score: 0.23499999474734068 Mean window score: 0.1771666627066831\n",
      "Episode: 3 Score: 0.4359999902546406 Mean window score: 0.24187499459367245\n",
      "Episode: 4 Score: 0.38749999133870006 Mean window score: 0.270999993942678\n",
      "Episode: 5 Score: 0.4584999897517264 Mean window score: 0.30224999324418605\n",
      "Episode: 6 Score: 0.5434999878518283 Mean window score: 0.33671427818813493\n",
      "Episode: 7 Score: 0.7414999834261835 Mean window score: 0.38731249134289103\n",
      "Episode: 8 Score: 1.0129999773576857 Mean window score: 0.45683332312231273\n",
      "Episode: 9 Score: 0.8734999804757535 Mean window score: 0.4984999888576568\n",
      "Episode: 10 Score: 1.161499974038452 Mean window score: 0.5587727147831836\n",
      "Episode: 11 Score: 2.2799999490380287 Mean window score: 0.702208317637754\n",
      "Episode: 12 Score: 1.730999961309135 Mean window score: 0.7813461363817064\n",
      "Episode: 13 Score: 1.8829999579116703 Mean window score: 0.8600356950624181\n",
      "Episode: 14 Score: 2.335499947797507 Mean window score: 0.9583999785780907\n",
      "Episode: 15 Score: 2.657499940600246 Mean window score: 1.0645937262044753\n",
      "Episode: 16 Score: 2.8444999364204704 Mean window score: 1.1692940915112986\n",
      "Episode: 17 Score: 3.178499928954989 Mean window score: 1.280916638035948\n",
      "Episode: 18 Score: 2.922999934665859 Mean window score: 1.3673420747006801\n",
      "Episode: 19 Score: 3.2459999274462463 Mean window score: 1.4612749673379584\n",
      "Episode: 20 Score: 3.8089999148622153 Mean window score: 1.573071393410542\n",
      "Episode: 21 Score: 4.324999903328717 Mean window score: 1.6981590529522774\n",
      "Episode: 22 Score: 3.9139999125152825 Mean window score: 1.7944999598897993\n",
      "Episode: 23 Score: 5.372499879915267 Mean window score: 1.9435832898908607\n",
      "Episode: 24 Score: 4.981499888654798 Mean window score: 2.065099953841418\n",
      "Episode: 25 Score: 5.327999880909919 Mean window score: 2.1905961048825144\n",
      "Episode: 26 Score: 4.815999892354012 Mean window score: 2.2878332821962735\n",
      "Episode: 27 Score: 5.442999878339469 Mean window score: 2.400517803487102\n",
      "Episode: 28 Score: 5.190499883983284 Mean window score: 2.496724082124901\n",
      "Episode: 29 Score: 6.514999854378402 Mean window score: 2.6306666078666847\n",
      "Episode: 30 Score: 5.717499872203916 Mean window score: 2.730241874458208\n",
      "Episode: 31 Score: 6.022999865375459 Mean window score: 2.833140561674372\n",
      "Episode: 32 Score: 7.129999840632081 Mean window score: 2.9633484186124845\n",
      "Episode: 33 Score: 7.361499835457653 Mean window score: 3.0927058132255776\n",
      "Episode: 34 Score: 7.076999841816724 Mean window score: 3.206542785471039\n",
      "Episode: 35 Score: 7.193999839201569 Mean window score: 3.317305481407998\n",
      "Episode: 36 Score: 7.548499831277877 Mean window score: 3.4316620854585356\n",
      "Episode: 37 Score: 7.27399983741343 Mean window score: 3.5327762368257694\n",
      "Episode: 38 Score: 7.879999823868275 Mean window score: 3.644243508288398\n",
      "Episode: 39 Score: 7.549499831255526 Mean window score: 3.741874916362576\n",
      "Episode: 40 Score: 8.27899981494993 Mean window score: 3.852536499254951\n",
      "Episode: 41 Score: 7.410999834351242 Mean window score: 3.9372618167572435\n",
      "Episode: 42 Score: 7.482499832753092 Mean window score: 4.019709212478077\n",
      "Episode: 43 Score: 7.935999822616577 Mean window score: 4.108715817253952\n",
      "Episode: 44 Score: 8.428499811608344 Mean window score: 4.204711017128494\n",
      "Episode: 45 Score: 8.499499810021371 Mean window score: 4.2980759908870345\n",
      "Episode: 46 Score: 8.800499803293496 Mean window score: 4.393872242214831\n",
      "Episode: 47 Score: 8.193499816861003 Mean window score: 4.473031150019961\n",
      "Episode: 48 Score: 8.411999811977148 Mean window score: 4.553418265570108\n",
      "Episode: 49 Score: 7.64199982918799 Mean window score: 4.615189896842465\n",
      "Episode: 50 Score: 9.849999779835343 Mean window score: 4.717833227881542\n",
      "Episode: 51 Score: 9.752999782003462 Mean window score: 4.814663353922348\n",
      "Episode: 52 Score: 8.654999806545675 Mean window score: 4.887122532273731\n",
      "Episode: 53 Score: 9.997999776527285 Mean window score: 4.981768407167316\n",
      "Episode: 54 Score: 10.753499759640544 Mean window score: 5.086708977212283\n",
      "Episode: 55 Score: 11.108999751694501 Mean window score: 5.194249883899466\n",
      "Episode: 56 Score: 11.229499749001116 Mean window score: 5.300131460480196\n",
      "Episode: 57 Score: 11.076999752409757 Mean window score: 5.399732637927257\n",
      "Episode: 58 Score: 11.26749974815175 Mean window score: 5.499186317761571\n",
      "Episode: 59 Score: 12.01349973147735 Mean window score: 5.607758207990167\n",
      "Episode: 60 Score: 11.661999739333988 Mean window score: 5.707008069159737\n",
      "Episode: 61 Score: 12.553999719396234 Mean window score: 5.8174434183571\n",
      "Episode: 62 Score: 11.592999740876257 Mean window score: 5.909118915539944\n",
      "Episode: 63 Score: 13.083499707560986 Mean window score: 6.021218615415274\n",
      "Episode: 64 Score: 13.068999707885087 Mean window score: 6.129646016837886\n",
      "Episode: 65 Score: 14.635499672871083 Mean window score: 6.258522587383844\n",
      "Episode: 66 Score: 14.206999682448805 Mean window score: 6.37715657387735\n",
      "Episode: 67 Score: 15.20549966013059 Mean window score: 6.506985148675192\n",
      "Episode: 68 Score: 15.266999658755958 Mean window score: 6.633941880705349\n",
      "Episode: 69 Score: 14.533999675139786 Mean window score: 6.746799849197269\n",
      "Episode: 70 Score: 16.413499633129685 Mean window score: 6.882950550379416\n",
      "Episode: 71 Score: 17.09099961798638 Mean window score: 7.024729009651733\n",
      "Episode: 72 Score: 17.30649961316958 Mean window score: 7.165575182302663\n",
      "Episode: 73 Score: 16.63449962818995 Mean window score: 7.293533620760599\n",
      "Episode: 74 Score: 17.3959996111691 Mean window score: 7.42823316729938\n",
      "Episode: 75 Score: 18.43999958783388 Mean window score: 7.5731248307274655\n",
      "Episode: 76 Score: 19.827999556809665 Mean window score: 7.732279047949312\n",
      "Episode: 77 Score: 18.76099958065897 Mean window score: 7.87367290093277\n",
      "Episode: 78 Score: 19.242999569885434 Mean window score: 8.017588428387867\n",
      "Episode: 79 Score: 19.49799956418574 Mean window score: 8.16109356758534\n",
      "Episode: 80 Score: 20.224499547947197 Mean window score: 8.310024505614498\n",
      "Episode: 81 Score: 20.0359995521605 Mean window score: 8.453024201304084\n",
      "Episode: 82 Score: 22.42999949865043 Mean window score: 8.621421494043197\n",
      "Episode: 83 Score: 21.239999525249004 Mean window score: 8.771642661081362\n",
      "Episode: 84 Score: 20.954499531630425 Mean window score: 8.914970388970174\n",
      "Episode: 85 Score: 23.00999948568642 Mean window score: 9.078866076141292\n",
      "Episode: 86 Score: 23.67099947091192 Mean window score: 9.246591747345553\n",
      "Episode: 87 Score: 21.750999513827264 Mean window score: 9.388687290146482\n",
      "Episode: 88 Score: 23.920999465323984 Mean window score: 9.55197169660915\n",
      "Episode: 89 Score: 22.816499490011484 Mean window score: 9.699355338758066\n",
      "Episode: 90 Score: 23.2519994802773 Mean window score: 9.848285494159375\n",
      "Episode: 91 Score: 24.18799945935607 Mean window score: 10.004151950302818\n",
      "Episode: 92 Score: 24.137499460484833 Mean window score: 10.156123428906925\n",
      "Episode: 93 Score: 25.358499433193355 Mean window score: 10.317850833207846\n",
      "Episode: 94 Score: 24.47149945301935 Mean window score: 10.46683660815323\n",
      "Episode: 95 Score: 24.6009994501248 Mean window score: 10.614067471090431\n",
      "Episode: 96 Score: 25.337999433651568 Mean window score: 10.765860584106525\n",
      "Episode: 97 Score: 25.04299944024533 Mean window score: 10.911545674475287\n",
      "Episode: 98 Score: 26.387499410193413 Mean window score: 11.067868439482542\n",
      "Episode: 99 Score: 25.7634994241409 Mean window score: 11.214824749329125\n",
      "Episode: 100 Score: 26.548999406583608 Mean window score: 11.479489743413401\n",
      "Episode: 101 Score: 26.990999396704137 Mean window score: 11.747259737428278\n",
      "Episode: 102 Score: 26.224499413836746 Mean window score: 12.007154731619174\n",
      "Episode: 103 Score: 29.6114993381314 Mean window score: 12.29890972509794\n",
      "Episode: 104 Score: 25.656499426532537 Mean window score: 12.551599719449877\n",
      "Episode: 105 Score: 27.59299938324839 Mean window score: 12.822944713384846\n",
      "Episode: 106 Score: 30.990499307308347 Mean window score: 13.127414706579406\n",
      "Episode: 107 Score: 28.917999353632332 Mean window score: 13.409179700281468\n",
      "Episode: 108 Score: 25.820499422866853 Mean window score: 13.65725469473656\n",
      "Episode: 109 Score: 29.444499341864137 Mean window score: 13.942964688350447\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 110 Score: 27.124999393709004 Mean window score: 14.202599682547152\n",
      "Episode: 111 Score: 28.65999935939908 Mean window score: 14.466399676650763\n",
      "Episode: 112 Score: 29.88899933192879 Mean window score: 14.74797967035696\n",
      "Episode: 113 Score: 29.4374993420206 Mean window score: 15.023524664198048\n",
      "Episode: 114 Score: 30.753999312594534 Mean window score: 15.307709657846017\n",
      "Episode: 115 Score: 28.857999354973437 Mean window score: 15.569714651989749\n",
      "Episode: 116 Score: 28.689499358739702 Mean window score: 15.828164646212942\n",
      "Episode: 117 Score: 30.57199931666255 Mean window score: 16.102099640090017\n",
      "Episode: 118 Score: 31.18799930289388 Mean window score: 16.384749633772298\n",
      "Episode: 119 Score: 32.18999928049743 Mean window score: 16.674189627302813\n",
      "Episode: 120 Score: 28.859499354939906 Mean window score: 16.924694621703587\n",
      "Episode: 121 Score: 31.04899930600077 Mean window score: 17.191934615730307\n",
      "Episode: 122 Score: 31.05149930594489 Mean window score: 17.463309609664606\n",
      "Episode: 123 Score: 32.10599928237498 Mean window score: 17.730644603689203\n",
      "Episode: 124 Score: 31.061499305721373 Mean window score: 17.991444597859868\n",
      "Episode: 125 Score: 30.462499319110066 Mean window score: 18.242789592241866\n",
      "Episode: 126 Score: 34.51399922855198 Mean window score: 18.539769585603846\n",
      "Episode: 127 Score: 30.440499319601805 Mean window score: 18.78974458001647\n",
      "Episode: 128 Score: 31.959999285638332 Mean window score: 19.057439574033022\n",
      "Episode: 129 Score: 32.06899928320199 Mean window score: 19.312979568321257\n",
      "Episode: 130 Score: 32.01299928445369 Mean window score: 19.575934562443752\n",
      "Episode: 131 Score: 31.864999287761748 Mean window score: 19.83435455666762\n",
      "Episode: 132 Score: 31.576499294210226 Mean window score: 20.0788195512034\n",
      "Episode: 133 Score: 33.26849925639108 Mean window score: 20.337889545412732\n",
      "Episode: 134 Score: 33.61099924873561 Mean window score: 20.603229539481923\n",
      "Episode: 135 Score: 33.39199925363064 Mean window score: 20.865209533626214\n",
      "Episode: 136 Score: 34.5349992280826 Mean window score: 21.13507452759426\n",
      "Episode: 137 Score: 32.34249927708879 Mean window score: 21.385759521991012\n",
      "Episode: 138 Score: 32.64899927023798 Mean window score: 21.63344951645471\n",
      "Episode: 139 Score: 33.4159992530942 Mean window score: 21.8921145106731\n",
      "Episode: 140 Score: 31.27899930085987 Mean window score: 22.1221145055322\n",
      "Episode: 141 Score: 32.959499263297765 Mean window score: 22.377599499821663\n",
      "Episode: 142 Score: 33.62449924843386 Mean window score: 22.63901949397847\n",
      "Episode: 143 Score: 32.802999266795815 Mean window score: 22.887689488420268\n",
      "Episode: 144 Score: 34.91199921965599 Mean window score: 23.152524482500738\n",
      "Episode: 145 Score: 35.98899919558316 Mean window score: 23.42741947635636\n",
      "Episode: 146 Score: 34.28799923360348 Mean window score: 23.682294470659457\n",
      "Episode: 147 Score: 35.0074992175214 Mean window score: 23.950434464666063\n",
      "Episode: 148 Score: 32.713999268785116 Mean window score: 24.193454459234143\n",
      "Episode: 149 Score: 34.8624992207624 Mean window score: 24.465659453149886\n",
      "Episode: 150 Score: 34.394499231223016 Mean window score: 24.711104447663764\n",
      "Episode: 151 Score: 34.376499231625345 Mean window score: 24.957339442159988\n",
      "Episode: 152 Score: 36.35549918739125 Mean window score: 25.234344435968442\n",
      "Episode: 153 Score: 34.51099922861904 Mean window score: 25.479474430489354\n",
      "Episode: 154 Score: 34.55499922763556 Mean window score: 25.7174894251693\n",
      "Episode: 155 Score: 35.43999920785427 Mean window score: 25.960799419730897\n",
      "Episode: 156 Score: 35.90599919743836 Mean window score: 26.20756441421527\n",
      "Episode: 157 Score: 34.122999237291516 Mean window score: 26.43802440906409\n",
      "Episode: 158 Score: 33.52349925069139 Mean window score: 26.66058440408948\n",
      "Episode: 159 Score: 34.99699921775609 Mean window score: 26.89041939895227\n",
      "Episode: 160 Score: 34.87449922049418 Mean window score: 27.122544393763878\n",
      "Episode: 161 Score: 36.59849918195978 Mean window score: 27.362989388389515\n",
      "Episode: 162 Score: 34.30399923324585 Mean window score: 27.59009938331321\n",
      "Episode: 163 Score: 34.350499232206495 Mean window score: 27.802769378559663\n",
      "Episode: 164 Score: 35.5959992043674 Mean window score: 28.028039373524486\n",
      "Episode: 165 Score: 34.84849922107533 Mean window score: 28.230169369006525\n",
      "Episode: 166 Score: 35.744499201048164 Mean window score: 28.445544364192518\n",
      "Episode: 167 Score: 35.58599920459092 Mean window score: 28.64934935963712\n",
      "Episode: 168 Score: 34.295999233424666 Mean window score: 28.83963935538381\n",
      "Episode: 169 Score: 36.09649919318035 Mean window score: 29.055264350564215\n",
      "Episode: 170 Score: 34.96499921847135 Mean window score: 29.240779346417636\n",
      "Episode: 171 Score: 35.58249920466915 Mean window score: 29.425694342284466\n",
      "Episode: 172 Score: 34.53549922807142 Mean window score: 29.597984338433484\n",
      "Episode: 173 Score: 36.84249917650595 Mean window score: 29.800064333916637\n",
      "Episode: 174 Score: 35.390499208960684 Mean window score: 29.980009329894557\n",
      "Episode: 175 Score: 36.35549918739125 Mean window score: 30.159164325890128\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from collections import deque\n",
    "\n",
    "def train(agent):\n",
    "    optimizer = optim.Adam(agent.parameters(), lr=LR)\n",
    "    \n",
    "    scores = []\n",
    "    scores_window = deque(maxlen=100)\n",
    "    score = evaluate_agent(agent)    \n",
    "    scores.append(score)\n",
    "    scores_window.append(score)\n",
    "    \n",
    "    e = 0    \n",
    "    print(\"Episode:\", e, \"Score:\", score, \"Mean window score:\", np.mean(scores_window))\n",
    "    while (np.mean(scores_window) < TARGET):\n",
    "        e += 1\n",
    "        r_states, r_actions, r_log_probs, r_returns, r_advantages = run_rollout(\n",
    "            agent)\n",
    "\n",
    "        dataset = TensorDataset(\n",
    "            r_states, r_actions, r_log_probs, r_returns, r_advantages)\n",
    "        loader = DataLoader(dataset, batch_size=BATCH_SIZE)\n",
    "\n",
    "        for _ in range(PPO_UPDATES):\n",
    "            for (states, actions, old_log_probs, returns, advantages) in loader:\n",
    "                dist = agent.dist(states)\n",
    "                values = agent.value(states)\n",
    "                entropy = dist.entropy().mean()\n",
    "                new_log_probs = dist.log_prob(actions)\n",
    "\n",
    "                ratios = (new_log_probs - old_log_probs).exp()\n",
    "                surr = torch.clamp(ratios, 1.0 - EPS, 1.0 + EPS) * advantages\n",
    "\n",
    "                loss1 = - torch.min(ratios * advantages, surr).mean()\n",
    "                loss2 = (returns - values).pow(2).mean()\n",
    "\n",
    "                loss = 0.5 * loss1 + loss2 - ENTROPY_BETA * entropy\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "        score = evaluate_agent(agent)        \n",
    "        scores.append(score)\n",
    "        scores_window.append(score)\n",
    "        print(\"Episode:\", e, \"Score:\", score, \"Mean window score:\", np.mean(scores_window))\n",
    "\n",
    "    return scores\n",
    "\n",
    "scores = train(agent)\n",
    "agent.save(\"ppo.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can plot the convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAEGCAYAAABiq/5QAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAA2pUlEQVR4nO3dd3hc1bXw4d8a9d6bZVX3Au7GmN5LSCiBQJJLgCQ4yQ3pH2nc3PQbUglJCIEEAiSUUEMNYMCAsY17L7JlSZYlq/cuzcz+/jhnxpIs2ZKt0Yyk9T6PHs3sOWdm6Uhas2effdYWYwxKKaUmDoe/A1BKKTW6NPErpdQEo4lfKaUmGE38Sik1wWjiV0qpCSbY3wEMRXJyssnNzfV3GEopNaZs3ry51hiT0r99TCT+3NxcNm3a5O8wlFJqTBGRQwO161CPUkpNMJr4lVJqgtHEr5RSE4wmfqWUmmA08Sul1ASjiV8ppSYYTfxKKTXBaOJXSqkAsau8iQ3F9T5/HU38Sil1Erqdbprae0b0OX/xn71841/bRvQ5B6KJXymlTsLv39rPlX9YPaLPWVLbTnljB1XNnQA4Xe4RfX4PTfxKqQmpuqWT6/68hsP17Se1/6aSBsobO2hs7x6ReDp7XBxp6gBgy6EGimvbWPLzt1h9oGZEnr83TfxKqQlpQ3E9W0obef8kEqvbbdhT0QxASd3w3jj2V7Ww7mDdMe1lDe14VsLdUtrAazsraGjvYUpK9LDjOxFN/EqpcWFDcT1X37eGti7nkLY/UNUKQEFlyzGPdfa4eHZz2aBDLYcb2mm1X6ektm1Ycf7itb184R+bjnnuklrrDSQ6LJgtpY28sqOChdnxTIqPGNbzD4UmfqXUuPDUhlK2H25kZ3nTkLYvrLYS/74BEv/PX93L/3tmOx8U1g64754jzd7bJXWDJ/4XtpbxzKbD9PRK8nsrWmjudLLtcGOfbT3Pc/ncdLYdbmRvRTMfOX3SkH6W4dLEr5Qa81xuw6qCagB290rKx3Og2kr4BZUtGM8YC/BuQTX/+NCqZuwZzulvT0UzQQ4hOTqMQ72GegqrW7n9sU00dfTQ0NbNt5/dwZ3P7uD8X79LSW0bDW3dVNonbt8tqKGpo4dvPb2dsoZ2DtW1ExsezIUzU3G5rXiuPC19mEdiaHxWj19EwoH3gTD7dZ41xvxQRB4BzgM8b8u3GmO2+SoOpdT4t7W0gQZ7auWeISR+p8tNcW0bcREhNHX0UNXcRXpcOF1OF995bgfT06Jp7nCyr+LYTwOe15iSEkVydFifHv8LW8tYuaeKpzaUEhUWTI/L8IOrZvN/r+3luS1lLJ+SDEBESBCrCqpxG8NzW8qYFB9OSV0buclRLMxOAGBxTgIZcSM/zAO+7fF3ARcaY+YB84HLRWSZ/didxpj59tc2H8aglJoA3tpbTbBDWJgdz+4jJx7qOVTfTo/LcPkcq0e9r9J6s1i1r4aq5i6+d+Us5mbGsvc4Pf7ZGbHkJEX16fGvKbRO2j627hDPbSljelo0nz0rl9My41h3sM77OjcuyWL3kWYeXlMMwMo9VRyqaycnKYr0uHA+dUY2/33BlJM/ICfgs8RvLK323RD7yxxnF6WUOilv763ijPxEluUnUVjdSpfTxaNrS9ha2jDg9p4Tux85PQOwZtqA1WNPjg7jnKnJzMqIpai2jc4eFwDGGP6+ppinNx6moqmT2ZNiyU2KpL6tm6aOHpo7e9hR1sicSbGUN3awtbSRaxdMRkRYlp/E9rJGtpQ2khQVyvWLJgPQ4zJ8+oxs9lW2cLihndykSAD+79rTuHBmms+Ol0/H+EUkSES2AdXASmPMevuhn4vIDhG5R0TCBtl3hYhsEpFNNTUjP49VKTU+VDR1cKC6lQtnpjFnUhxOt+GV7RX88KXd/H1NCQDri+q4+aH13ittC+3x/UU5CaTFhrGvsoWGtm7e2VfNNfMnERzkYGZ6LC638Z4ELq1v58cv7+Hbz+0AYJbd4wcorWtnY3E9bgPfv3IWkxMiEIGr51snZ5flJ9LjMryxq5KZGTHMmRRLfnIUNy/L4fZz8gEwBu/z+ZpP19w1xriA+SISD7wgInOB7wGVQCjwIPAd4CcD7Pug/TiLFy/WTwpKTQBOl5sghyAiQ97HMx3ztMw4UmKsfuTPXt0DWEM6AG/vq2b1gVr+96Vd3HvTAg5Ut5IZH0FUWDAz0mPZW9HCvzYdpsdluG6h1RuflREDwN6KZuZmxrHF/vTw06vn0NzpZFl+EgdrrDeFkro2tpY2EhbsYFFOAj/+2Bz2VbZ4p2IuyU0kyCF0u9zMTI9FRHjzG+fiEMHhEKanRbO/qpUcu8fva6Myq8cY0wisAi43xlTYw0BdwN+BpaMRg1LKd97bX8MV966m23nyJQZaOntY8vO3eGFr+bD2K6qxTq7mp0SRkxhJdFgwDe09OAQO2SdePdu8uO0If363kJ3lTUxNtS6Mmpkew96KZu7+zz5mZcQye1IsYPW+w0Mc3umeW0sbiQoN4lNn5PDlC6YSEuQgO9FK1CW1baw9WMuS3ETCQ4K4aFYaX75gqjfGqLBgTp8c5309gOAgBw6H9QZ36ex0RCB3rPf4RSQF6DHGNIpIBHAJ8EsRyTDGVIj1ln4NsMtXMSilRsfmQw3srWimuqWTyQkn12tdU1hLQ3sPW0obvL3uoSiqbSUuIoSkqFBEhFkZMWw61MAnl2bz+PpSGtu7Kapt5eJZqTS09/Cr1wsAvCd2b1meS2x4MOlxEZw5Jcn7vEEOsT8NWCdkt5Y2Mi8rniDH0U8jkaHBpMWG8duV+wH49uWDz7tflp/E1tJGZmXEHvPYf18wheVTk7yfWHzNl0M9GcCjIhKE9cniaWPMKyLyjv2mIMA24Is+jEEpNQoa2rrt7z1MTji553hvv3Uuz3MF61AV1bSRnxLlHR763Nl5XDQrjfzkKB5fX8rBmlZK69q5bE4637h4OoXVrbR3O5kzyeqBZ8ZHcMeF0wZ87lnpMby+u5Lmzh72VjTzhfPyj9nmzstmsqu8ifyUqOO+YX1qaTY9Tre3x99bZGiwd6rnaPBZ4jfG7AAWDNB+oa9eUynlH/V24q8/yYJlxhjeLbASf/EwSyAcrGnl7Kkp3vuXz7Vm6hywZ+qsPlCL023IS44iNNjhHcoZisvnpvPUxsPc9cIunG7Dgqxj39WuXzTZO0vneLISI/mfq2YP+bV9yacnd5VSE4M38bd1DWs/Ywzt3S7KGzuoaOpkckIEZQ0ddPa4CA8JGnS/zh4XHd0uQoIdVDV3kZ9y7Nh4VmIkIrDKfkOZMsA2J3Le9BSW5Sfy8vYjAMzPjh/2cwQiLdmglDplRxP/0BcmcbrcfOmfW1j0s5X8+OXdANxyZi7Qt/6Ny236lFQA+NYz27nyD6u98+8HSurhIUFkxIazo6wRgLzk4Ve5FBG+e8UsALITI0mOHp0xeF/TxK+UOmWeIR7PWP+JuN2Gbz+7g9d3VzIlJZo1hXXMSIvxnlz1VLw0xnDDX9by45f3ePctqGzh1R0VVDR18rfVRQCDli7OSYrCGIiPDCExKvSkfrb5WfF84bx8bl6Wc1L7ById6lFKnRJjjDfh1w0x8X9QWMvzW8v5xsXT+epFU3ltZyXpcWHkJls99yI78a+3a+Z396pued+qQqJCg4gKC+a1nZU4BLIHmf+emxzJuqI68pNPbZrk9+xe/3ihiV8pdUqaO5047WqSQ+3xb7dLEn/27FxExFs6ASA1Joxie979Y+tKACiuacMYQ0ldO6/sOMLt5+YTFuTgD+8UkpUYSVjwwOcDPFfCnswwz3imQz1KqVNS3yvZ1w8x8e+paCYnKZKY8JBjHstNjqK4to2Kpg7e2F1FSkwYbd0uqpq7eHtvFW4Dty7P5YbFWYhw3N68p/bNQCd/JzJN/EqpU+JJ9pGhQUOezrn7SDNzBplWmW8n/l+/XoDbGL51yXQAimpa2X2kmdSYMDLiIshKjOQ7l8/kM8tzB32dWRmxBDnEe9WssuhQj1LqlHgS/5SUaI40dpxw++bOHkrr27lxSdaAj+clR1HX1s3zW8v56kXTOG+GNUe/qLaN3UeamJt5NIl/8bzjly7OSYpi/fcvGjezcUaK9viVUscoqmll7SDLDvbnGdefmhpNQ3s3bvfxayp6FkoZ7EIqT/udl83gm5dMJz02nIiQIPZUNHOwpm3QTwqD0aR/LO3xK6WO8b8v7mZfZQub/ufiYx776St7KKhs4Z+fPwM4OpNnSkoUbgNNHT3Ut3fz9KbDrDtYR3psOPOy4rlxSRbJ0WHepREHS+BnT01mw/cvIjU2HLDm0uclR/Hm7kpcbjPsxK+OpYlfKdVHQ1s364rqcBtDj8tNSNDRgQGny81zW8pobO/hcH07WYmRNLR3Exbs8BZnq27p4sYH19Ha6WRhdgKFNa28uaeKP7x9gNvPyae8sYOUmDBSY8IHfH0R8SZ9j/yUKO/6t54aO+rkaeJXSvWxcm+Vd7HvutZu0uOOJuHNhxpotBczeWtvFbedlUd9WzeJUaHeC6TWHaylsb2He2+az9XzMwGrns4f3z7An1YVImKVQhiOfPsCrbiIECYn+GYd2olEx/iVUn28savSe7u6pbPPYyv3VBFq16FfuacK4JjE/45dG8ezaDhYJ37vuXE+37tiJsYwYLGz4/FM2ZydETusRVrUwLTHr5TyaunsYfWBWpbkJrCxpIHq5qNF14wxrNxbxZlTkpgzKZYH3i+isb3bm/gT7MT/YVEdSVGhx/TMRYQvnDeFi2alkpU4vJr9nnn4czN1fH8kaI9fKeX12s4Kul1ubraLpVW3WIn//f01PLK2hEN17Vw8O41LZqfhchtWFVRT39ZNQmQoiZFW4u92upmXFT9oz3xqasygV9oOZnpaDEtyE7jMXjxFnRrt8Ss1Rh2ub+fq+9bw1IplTE87dnGP4epyuvjD24WcPjnOuzpVdUsnRTWtfObhDQCEBju4ZFYaqTFhTIoL57F1h7w9/ojQICJCgujocTE/K/6U4+ktPCSIZ764fESfcyLTHr9SY9Teimbq27rZUFw/rP2ONHbw1Se30trl7NP+5PpSyhs7uPOyGYQGO0iMCqWmpYvCamtB8QdvXsT6711Eelw4Dofw1YumsbW0kdYup3d83/N93ggnfjWyNPErNUbVtFrDMAdrWoe134vbjvDS9iNs7PWG0eV08adVhSzLT+TsqdYSgKkxYVS3dHlXxDojP8k7jg/WylOeOviehJ8QZdXemT85/uR+KDUqfJb4RSRcRDaIyHYR2S0iP7bb80RkvYgUisi/ROTkimQrNcHV2OPvRTXDW6pwfXEdgHcRE4D9la3UtnZz87Jc79h8Sq/EnxwdSlxE34JqwUEOvnP5TAAmxVtTPtNiwpmSEkVc5LHF11Tg8OUYfxdwoTGmVURCgA9E5D/AN4F7jDFPichfgM8B9/swDqXGJc+J1+H0+J0uN5tKGgAo6JX4PbdnZhw9V5ASE8bB6lbCgh3kJg1c3fLSOem8+tWzmZluzbb5wVWz6XS6hveDqFHnsx6/sXj+IkPsLwNcCDxrtz8KXOOrGJQazzw9/vLGDjq6j59se1xuelxu9lQ009rlJNghfXv8VS2EBjvI6TXNMjUmnJrWLopq2sg7TunjOZPiCHJYnxJyk6O8bwIqcPl0Vo+IBAGbganAfcBBoNEY4zmrVAZkDrLvCmAFQHZ2ti/DVGpMqmnpQgSMgeLatkGLngHc+vcN9DgNF8xMBeDyuem8ZV+hG+QQCipbmJoSTXCv8gypMWH0uAy1rV3kaT37ccWnJ3eNMS5jzHxgMrAUmDmMfR80xiw2xixOSRne5d1KjTdlDe389f2iPouO17R0McvuXR9vuKepo4d1B+vYUFLPH94+QE5SJOdOS6Gzx83h+nbA6vHPSO87JTQ19mhVy1NdulAFllGZ1WOMaQRWAWcC8SLi+aQxGSgfjRiUGste2FLOz1/bS22rVQnTGENNSxdL8xIROZr4jTH88vV97Cxr8u67vqgOt7HKHXT0uDgjL5HpdpIvqGqhqaOHiqbOY64F6F1ETZcuHF98OasnRUTi7dsRwCXAXqw3gOvtzW4BXvRVDEqNF56pm55FT5o7nHS73ExOiCArIZKD9sye1i4n9797kOe3lnn3XXuwjvAQB//8/BlcNieNGxZnMS3VSuQHqlo4YI/1z0jvm9xTY472+HMGWcxcjU2+HOPPAB61x/kdwNPGmFdEZA/wlIj8DNgKPOTDGJQaFzw1c+rauoAYb/G0lJgwpqREcdC+yMrzieBQXbt33w8Ka1mal0RiVCgP3LzY2z45IYKCqlbv3Pxjevz2UE9mfAThIcMrsaACm88SvzFmB7BggPYirPF+pdQQ9e/xe2b0pMaEk58SbdXPd1snYgFK6qxPAFXNnRRWt3LDosnHPOf0tBh2ljUSGuQgKjSIzPi+RdUiQ4OJDgs+7oweNTbplbtKjQGeRO9N/HaCT4kJIyshgs4eN/Xt3dTa2x2ub8flNqw9aC2feJZ9NW5vl81Jo6Sunee2lDE9PWbAompXzE3nsrlaGG280SJtSgU4Y4x3aKfOHsrxDP2kxIR5F0qpau709vh7XIaKpg42FNcTGx7M7Ixjp3reuCSb2Rlx3P9eIedOG3jm3K9vmDfiP4/yP038SgW41i4nnT1uoG+PPyzYQWx4sHeZwqrmTmrsNwawxvm3HW5iXlY8DsfAJZJPmxzHnz+9yMc/gQo0OtSjVIDzlGYAqG8/OsafEhOGiJBuJ/7Kpi7qWru8V9HurWhmf1XLiJdIVmOfJn6lApxnfN8hUO8Z6mnpJMWebmm9AUClPdSTlxxFaLCD13ZW4HIb5mmlTNWPDvUoFeA8Pf7cpKg+s3o8s21CghwkRYVR3dxJbWu3d/79ltJGAE7Pihv9oFVA0x6/UgHO0+OfmRFDXZunx9/l7fEDpMeFeXv8ydFh5NoXXGXGR/S5Alcp0MSvVMCrbukkNMgqjdzQ3k1jezeN7T1MTjh6NW16bDiVTZ3UtliJPzvR+jQwT3v7agCa+JUKcJ4TuUnRYbjchs2HrHr609OOllhIiw3ncH07bd0ukqJDyU223hR0fF8NRMf4lQpQr++qYEZ6LDUtXSTHhJFkl1b4sMhaQWta6tESC2mx4bTZNflTosO8ZZSX5iWOctRqLNDEr1QAemN3JV/85xaW5CbQ0ukkKzHSu67th0X1RPYrseCZ0gmQHBPKktxE3rvzfHIGWTlLTWw61KNUgClv7ODbz+4gPMTBxpIGDta0khIT5k38u480MTU1us9FWWlxvRJ/tHXSV5O+GowmfqUCwL7KZtxua5GVn768B6fLzb9WnEl4iIMelyE1JoykaCvxu03fYR7o1+OPDkOp49HEr5SfFVa3cvnvV3P/ewepaOrgzT2VfGZ5LvOy4rl2gbUyae8eP/Q9sQuQ1mu1LM8bhFKD0TF+pfxsnX2y9oH3DlLT0oUBPrXUWmf61uV5/HvrEWamxxIWHER0WDCtXc5jaufHRYQQFuywv7R2vjo+TfxK+dmG4nqiw4Jp7nTyyNoSLpyZSlaiNR1zRnoMO390qXcR9MSoUFq7nEzr1+MXEdLjwgkaoLSyUv1p4lfKj4wxbCyu54KZqbjdhld3VvBfy7L7bONJ+mAl/trWLibFRfR/Kl0QXQ2ZJn6l/OhwfQeVzZ0szUvkkllpzM+K5/zpqYNuPy01mpjw4AHLLP/uE/N9GKkaT3yW+EUkC3gMSAMM8KAx5l4R+RFwO1Bjb/p9Y8xrvopDKX/7y3sHyYgL5+r5md629UV17CxvIjYiBICluYmkx4Vz+7n5x32u/7vuNNzGDPhYQpSe1FVD48sevxP4ljFmi4jEAJtFZKX92D3GmN/48LWVCgidPS5+t3I/iZGhXHX6JIIcgttt+M5zOyipaycqNIj4yBCmpUaf+MmwKnEqdap89ldkjKkwxmyxb7cAe4HM4++l1Piy+VAD3U43lc2d3lIL7+yrpqSunYtmptLW7WJJbuKgK2Qp5QujMsYvIrnAAmA9cBZwh4h8BtiE9amgYYB9VgArALKzs/s/rNSYsKawlmCHEB4SxPNbyjlrajIPrykmIy6cv9y8iK2ljWQmHHuiVilf8vnnRhGJBp4Dvm6MaQbuB6YA84EK4LcD7WeMedAYs9gYszglZeCFoJUKdGsKa1mQHc9HTsvg9V0VPLq2hLUH67hleS4hQQ6W5iX2qbmj1GjwaeIXkRCspP+4MeZ5AGNMlTHGZYxxA38FlvoyBqX8pam9hx3lTZw1NZlrF2bS1u3ihy/tZs6kWD65RD/FKv/x5aweAR4C9hpjfterPcMYU2HfvRbY5asYlPKHzh4Xz24uo7KpE2PgrKnJLM5J4BfXncbU1GgW5yQgeqGV8iNfjvGfBdwM7BSRbXbb94FPish8rCmeJcAXfBiDUqPuifWl/OSVPQDEhAczPyseEeGTS7WXrwKDzxK/MeYDYKBujc7ZV+PGE+tLeWxdCa9+9RyC7Jk5L+84wsz0GH5x3WlEhwXrFEwVcPQvUqlhMMbwmzcK2H2kCYC1B2vZV9nCtsPWxLTD9e1sLW3k6vmZLMhOYFq/YmpKBQJN/EoNw8o9VfxpVSFPbzwMQHFtG2DNzQertw9w1ekZ/glQqSHQxK/UEBlj+OM7hQDsr2rFGNMr8VsVSF7eXsHC7HhvdU2lApEmfqWG6N39NewsbyIxKpQD1S1UNXfR3u0iOzGSvRXN3LeqkL0VzVy7cLK/Q1XquDTxKzVEj64tYVJcOLefk09tazebD1nj+redlQvAr98oYFl+oncRFaUClSZ+pYZoz5FmzpySzOxJsQC8sbsSgEvnpDM5IYLEqFDuvWmBd3aPUoFK6/ErNQTNnT1Ut3QxNTWaGfZMnVX7qgkPcZARG84DNy8iNMhBWq9Fz5UKVNrjV6ofYwxX/+kDHl9/yNtWVGOdxJ2SEkVabBgx4cG0dDnJTYrC4RDmTIrTqZtqzNDEr1Q/je09bC9r4oUt5d62wupWAKakRiMi3sXO81N0uUM19mjiV6qfQ/XtAGw93EhzZw8AB2taCQkSsu1pmtPtxc7zdJ1bNQZp4leqn0N11rCOy2348KC1eMrB6lZykqK85RempVo9/rzkoa2cpVQg0cSvVD+H7R5/eIiD1QdqAavHP6XXsM7i3ASCHMLpk+P8EqNSp0Jn9SjVz6G6dlJjwpibGccHhbX0uNwcqmvnsjnp3m1OnxzP9h9eSnSY/gupsUd7/Er1U1rfTk5SJOdMS6a4to139lXjdBum9lsQXZO+Gqs08SvVT2l9O1mJkVw6J53osGC+8sRWAKak6Hi+Gh808SvVS2ePi8rmTnISo8iMj+DpL5xJQlQIQQ7RqZtq3NDPqkr1UtbQgTGQnWQtgD57Uiwv33E2JXXtxISH+Dk6pUaGJn6leimtt6ZyZice7d2nxoaTqqUY1Dgy5KEeEYkQkRnD2D5LRFaJyB4R2S0iX7PbE0VkpYgcsL8nnEzgSo0kYwwtnT2U1llTOXOStJ6+Gr+GlPhF5KPANuB1+/58EXnpBLs5gW8ZY2YDy4Avi8hs4LvA28aYacDb9n2l/OqhD4qZ/5OVPLSmmMjQIJKiQv0dklI+M9Qe/4+ApUAjgDFmG5B3vB2MMRXGmC327RZgL5AJXA08am/2KHDN8EJWamR1O938dXURiVGhlDd0MCXFqsej1Hg11DH+HmNMU79/BjPUFxGRXGABsB5IM8ZU2A9VAmmD7LMCWAGQna0LW6hTt6qgmv95YRevfvVs4iOP9uhf3XmEquYuHrltCVmJkQRp0lfj3FB7/LtF5FNAkIhME5E/AmuHsqOIRAPPAV83xjT3fswYYxjkDcQY86AxZrExZnFKSsoQw1RqcG/urqS8scO7MDpYY/t/W13MtNRozpuewpSUaHK18Joa54aa+L8CzAG6gCeAJuDrJ9pJREKwkv7jxpjn7eYqEcmwH88AqgfbX6mRtLHEWirx7b3Wn1xnj4vvPb+T3Uea+fw5eTq8oyaMEw71iEgQ8Kox5gLgrqE+sVj/RQ8Be40xv+v10EvALcDd9vcXhxWxUiehoa2bwupWwoIdvLe/hvZuJ7c8vIGNJQ18+YIp3LAoy98hKjVqTtjjN8a4ALeIDLcM4VnAzcCFIrLN/roSK+FfIiIHgIvt+0r5lGdh9FuX59La5eT2xzaxsaSB331iHndeNhOHrpOrJpChntxtBXaKyEqgzdNojPnqYDsYYz4ABvtvumjIESo1AjYeqickSPjS+VN4dF0JawrruHr+JK5bONnfoSk16oaa+J+3v5QakzaXNDA3M474yFAunJnKxpIGfvTROf4OSym/GFLiN8Y8KiKhwHS7qcAY0+O7sJQaOZ09LnaUNXHrWbkA/Or6eXT1uEjQi7TUBDWkxC8i52NdbFWCNXyTJSK3GGPe91lkSo2QjSX1dLvcLM1NBKw6+lpLX01kQ/3r/y1wqTGmAEBEpgNPAot8FZhSI+XN3VVEhARx9rRkf4eiVEAY6jz+EE/SBzDG7Ae0Rq0KeMYY3tpbxTnTkgkPCfJ3OEoFhKEm/k0i8jcROd/++iuwyZeBKTUSdpU3U9HUyaW91stVaqIb6lDPl4AvA57pm6uBP/skIqVG0Jt7KnEIXDgz1d+hKBUwhpr4g4F7PVfg2lfzhvksKqVOUk1LFxuK67libjoi8PquSpbkJpKoM3iU8hrqUM/bQESv+xHAWyMfjlIn75UdR7jknvf48hNbeHNPJRtLGjhQ3co1CzL9HZpSAWWoPf5wY0yr544xplVEdIkiFTB2lDVyxxNbmZ8VT314N39aVUhOYhSx4cFcM18Tv1K9DbXH3yYiCz13RGQx0OGbkJQavvtWFRIbHsw/PreUOy6Yyq7yZl7dWcGNS7KICNXZPEr1NtTE/zXgGRFZLSKrgaeAO3wXllJDd6CqhTd2V3Hr8lxiwkO4ZkEmmfERiMDNy3L9HZ5SAWeoQz15WCtoZQPXAWcwjBW4lPKl+987SERIELeeZa0GGhrs4O6Pn8bB6layddF0pY4x1B7/D+zVs+KBC7Cmct7vq6CUGipjDKv2VXPlaRl9Zu6cMy3F+0aglOprqInfZX//CPBXY8yrgM6PU35X29pNQ3sPcybF+jsUpcaMoSb+chF5ALgReE1Ewoaxr1I+s7+qBYAZ6TF+jkSpsWOoyfsTwBvAZcaYRiARuNNXQSk1VAWVVuKfnqaJX6mhGmo9/nZ6LcRijKkAKnwVlFJDtb+qhcSoUJKjdeRRqaHy2XCNiDwsItUisqtX249EpLzfGrxKDVl1cyf/2VnBPz48hNPlZl9lC9PTohHRNXOVGipfrkbxCPAn4LF+7fcYY37jw9dV41R9WzcX/+49mjudAESFBnGgqoUbFmf5OTKlxhaf9fjt1bnqffX8auJZuaeS5k4nD9y8iCkpUfzmjQLaul06vq/UMPljZs4dIrLDHgpKGGwjEVkhIptEZFNNTc1oxqcCiDGGbqcbgP/sqiQrMYJLZ6fx2bPzONLUCcCM9Gh/hqjUmDPaif9+YAowH+vk8G8H29AY86AxZrExZnFKSsoohacCzV9XF7H87nfYV9nMmsJarpibgYhw3YLJxEdai8BN0x6/UsMyqonfGFNljHEZY9zAX4Glo/n6auxZfaCW2tYubnzgQ3pchsvnWitpRYQG8ZULp3He9BRiw3UVUKWGY1QTv4hk9Lp7LbBrsG2VMsaws7yJ/OQomjp6SIsNY/7keO/jnzs7j0c/q30HpYbLZ7N6RORJ4HwgWUTKgB8C54vIfKwCbyXAF3z1+mrsK61vp7G9h29fNpO2LicpMWE4HDptU6lT5bPEb4z55ADND/nq9dT4s72sCYDTJ8cxNzPOz9EoNX5ovR0VsHYcbiQs2KF1eJQaYZr4VcDaUdbE7EmxhATpn6lSI0n/o1RAcrkNu440Ma/XyVyl1MjwZckGpYbNGMP3X9hJcW0b7d0uTp+sY/tKjTTt8auAUlrfzpMbDlPR1MmC7HjOnpbs75CUGne0x68CytbSRgD+8l+LmJWhq2op5Qva41d+9/quSj79tw9xutxsO9xIZGiQFl5Tyoe0x6/87sH3D7KltJF1RXVsLW3g9MlxBOmFWkr5jPb4lV+V1LaxxR7eeX5LOXsqmpmfNWjRVqXUCNAevxp13U43N/xlLWdOSSYs2IEILMtL4t/byjEGFmTH+ztEpcY1Tfxq1L268wjby5rYXtZEaLCD5VOSuHV5HuuK6gBYkBXv3wCVGud0qEeNKmMMf19TwpSUKK5bkEm30821CyZzzrRkYsKCyYyPIDU23N9hKjWuaY9fjaotpQ3sKGvip9fM5aYlWVy3cDLLpyThcAj/77IZ6DldpXxPE7/ymfq2bv69tZw1hbWcPyOFmRmx/ODfu4gND+bjCzMJCXL0uUDrluW5/gtWqQlEE7/yicLqFm74yzoa2q0FVN7eVw1AcnQYv/3EfCJD9U9PKX/R/z414iqaOvjMQxsIcjh4+Y6zmZsZy/riegoqW7huYSYxulSiUn6liV+NuO89v5PmTidPrVjmXUBlWX4Sy/KT/ByZUgp8OKtHRB4WkWoR2dWrLVFEVorIAfu7XqkzzhysaeXdghpWnJuvq2YpFaB8OZ3zEeDyfm3fBd42xkwD3rbvq3HksbUlhAY5+OTSbH+HopQahM8SvzHmfaC+X/PVwKP27UeBa3z1+mr0tXT28OzmMq46PYOUmDB/h6OUGsRoX8CVZoypsG9XAmmj/PpqhNS0dLH2YC2dPS5v26s7Kmjrdum0TKUCnN9O7hpjjIiYwR4XkRXACoDsbB02CDR3/2cfz20pIzI0iJ9fO5drF0xmXVEdqTFhumqWUgFutHv8VSKSAWB/rx5sQ2PMg8aYxcaYxSkpKaMWoBqaA9UtzEyPYXJCBA+8V4Qxhg3F9SzJS0REL79VKpCNduJ/CbjFvn0L8OIov74aAcYYimvaWJqXyA2LsthX2cKG4noqmjo5Iy/R3+EppU7Al9M5nwTWATNEpExEPgfcDVwiIgeAi+37aoypa+umpctJXnIUF8xMBeDu1/cBsFQTv1IBz2dj/MaYTw7y0EW+ek01Oopr2wDITY5iSkoUOUmRbC1tJC4ihOmpumSiUoFOyzKrQb2y4wj3rNyPMX3PwXsSf15SFCLCBTOsXv+S3AQcWl5TqYCniV8dwxjD31YXcccTW7n37QN8UFjb5/Hi2jaCHcLkhAgALpzpSfw6zKPUWKC1epRXl9PFXS/s4t2Campbu7l8Tjo7y5v45ev7mJEew2NrD/HJM7IpqW0jOzGS4CCr33DW1GTuunIWNyye7OefQCk1FJr4J5jOHhefeXgDty3P5YrTMvo89qd3Cnl2cxlXz5/EWVOSuW5hJi9uO8K3ntnOeb96l44eF5XNnRTXtpGXHOXdL8gh3H5u/mj/KEqpk6SJf4J5afsRNhTX0+V090n8u8qb+PO7B7luYSa/+8R8b/s1CzL55/pDGAMpMWG8tP0IYPXylVJjkyb+CWB9UR3PbSnjB1fN5uEPinEIbD/cSGF1K1NTo+lyurjz2R0kRIbyv1fN7rNvkEN4/kvLEREKKltYuacKoE+PXyk1tujJ3XGux+Xmu8/v5OlNZVz1xw/YV9nCty6dQZBDeG5LGQC/fr2AvRXN/PLjpxEfGXrMc3iuxJ2RHsOyfOsEriZ+pcYu7fGPQ0caO7j17xtYkpvI9LQYimvbuHlZDk9sKCU5OozPn5PH5kMNPLe5DIfA3z4o5uZlOVw068Q18/77/KkU125ndkbsKPwkSilfkP5ztAPR4sWLzaZNm/wdRsAxxvCvjYf5/VsH+MpFU/n0GTk0tHVzwwPrOFzfTpfTDcDinASe+eKZrC+uJyTIwaKcBF7fVckX/7kZEVg+JYmHbllCeEiQn38ipdRIEpHNxpjFx7Rr4h+77nxmO89sLiM5OpTa1m4umpnKtsONtHQ5efS2pRxp7OC3bxZw36cXsiC772Jnxhh2H2kmOymSWF0DV6lxabDEr0M9Y1RlUyfPbC7j5mU5/OCq2fzkld38e+sRzpuewmfPzmVRjjUW//FFA8+tFxFdGlGpCUoT/xi1qsCqaH3zmTmEBjv42TWn8dOr52pJZKXUCemsnjHAGMObuyv7rHb1zr5qMuMjmJYa7W3TpK+UGgpN/GPAzvImVvxjMw++XwRYpRXWFNZywcwUTfZKqWHTxD8GfFhUB8BTG0pxua2Vrtq7Xd7iaEopNRw6xj8GrC+qJ8ghHGnqZNW+al7bWUFYsIMz87VsglJq+DTxBziX27ChpJ5r5mfy/oEavvXMdpo6evj82XlEhOq8e6XU8OlQT4DbV9lMS6eTs6clcdOSLG/Sv+sjs/wdmlJqjPJLj19ESoAWwAU4B7rAYCJo6eyhvq2bnCSr7o0xxnuy9oMDtWwvayQkyLp/Rl4SHzltEudOT2FxToKe1FVKnTR/DvVcYIypPfFm49cvX9/HM5vKePoLZ+J0u/nCPzbz3StmcdXpGXzz6W1Ut3QR5BCyEiOYFG+tdqWrXCmlTpWO8fuRpy7+7Y9toqPHRUunkx+/vJv9VS1Ut3TxpfOn8OjaEu+atkopNRL8NcZvgDdFZLOIrBhoAxFZISKbRGRTTU3NKIfne82dPRyobuWj8ybR1uUkOiyYxz9/Bl1ONw++X8Q505L5zuUzWf/9i/j+lTqer5QaOf7q8Z9tjCkXkVRgpYjsM8a833sDY8yDwINgFWnzR5C+tK20EWPgxsVZfOuS6USHB5McHcY3Lp7Ob98s4JuXTAcgRguoKaVGmF8SvzGm3P5eLSIvAEuB94+/1/iytbQREZiXFdcnuX/p/CncuCSLxKhjF0RRSqmRMOpDPSISJSIxntvApcCu0Y7DH5wuN/e+dYCCyha2lDYwPTVmwB69Jn2llC/5o8efBrxgT0cMBp4wxrzuhzhG3SNrS7jnrf38a2MpLV1Orjo948Q7KaXUCBv1xG+MKQLmjfbr+ltZQzu/fXM/87Li2VvRTLfTfcziKEopNRr0yt1R8qOX9gBw36cW8H/XnkZkaBBn5if5OSql1ESk8/hHwfv7a3hrbxXfuXwmkxMiuX5RJNcuyCTIoVffKqVGn/b4fczpcvPTV/aQkxTJZ8/O9bZr0ldK+Ysm/hFmjOHFbeUcqmsD4M/vHuRAdSt3XTmLsGCtpqmU8j8d6hlBxhju/s8+Hni/iJiwYK6al8GTGw7zsXmTuGR2mr/DU0opQBP/iGnrcvLrNwp4ZG0J1y+azL7KZp7ccJiPnJbB7z4xT6tpKqUChib+EbCmsJZvPr2NquYubjkzhx9+dA7dLjdrCms5b3oKwUE6oqaUChya+E/Re/truP2xTeQmRfLnTy9iUY41Nz/cEcRFs3R4RykVeLQrOkSF1S385o0CXO6j9eK2HW7k9sc2MTUlmn+tONOb9JVSKpBp4h+ihz4o5k+rCnlyQykArV1OvvbUVlKiw3j882eQoPV1lFJjhCb+ITDGsGqftSbAb94s4HB9O999bgeH69v5/U3zNekrpcYUTfxDsK+yhcrmTm47K5eWTifn/noVr+yo4BsXT9elEJVSY46e3B2CVQXVAHzxvClkxIWz50gznz8nn7mZcX6OTCmlhk8T/xC8u6+GOZNiSYsNZ8W5U/wdjlJKnRId6jmBquZONpc26ILnSqlxQ3v8/Thdbn75+j4K7YXQf//WAUKChI/Nn+Tv0JRSakRo4u+loa2brz61ldUHakmIDGFVQQ2JUaE8efsypqfF+Ds8pZQaEZr4gfq2bv6zq4LfvFFAa5eTX338dK5dmMmqfdXMyoglKzHS3yEqpdSI8UviF5HLgXuBIOBvxpi7R+N1jTG8sbuS57eUkx4XjtNt2Fhcz4HqVgAW5yTws2vnMjM9FoBL56SPRlhKKTWqRj3xi0gQcB9wCVAGbBSRl4wxe3zxeo3t3by47QgHqlsorG7lw6J60mPDWXewDgMsykngmgWZLM1LZFF2Ag5dIEUpNc75o8e/FCi0F11HRJ4CrgZGPPH/4e0D3LeqkC6nm4TIEBIiQ7nrylncdlaudwUsLZeslJpo/JH4M4HDve6XAWf030hEVgArALKzs0/qhSbFR3D9osl86oxs5kzSi62UUgoC+OSuMeZB4EGAxYsXmxNsPqDrF03m+kWTRzQupZQa6/xxAVc5kNXr/mS7TSml1CjwR+LfCEwTkTwRCQVuAl7yQxxKKTUhjfpQjzHGKSJ3AG9gTed82Bize7TjUEqpicovY/zGmNeA1/zx2kopNdFpkTallJpgNPErpdQEo4lfKaUmGE38Sik1wYgxJ3Vt1KgSkRrg0EnungzUjmA4vjaW4h1LscLYincsxQpjK96xFCucWrw5xpiU/o1jIvGfChHZZIxZ7O84hmosxTuWYoWxFe9YihXGVrxjKVbwTbw61KOUUhOMJn6llJpgJkLif9DfAQzTWIp3LMUKYyvesRQrjK14x1Ks4IN4x/0Yv1JKqb4mQo9fKaVUL5r4lVJqghnXiV9ELheRAhEpFJHv+jue3kQkS0RWicgeEdktIl+z238kIuUiss3+utLfsXqISImI7LTj2mS3JYrIShE5YH9PCIA4Z/Q6fttEpFlEvh5Ix1ZEHhaRahHZ1attwGMplj/Yf8c7RGRhAMT6axHZZ8fzgojE2+25ItLR6xj/ZTRjPU68g/7uReR79rEtEJHLAiDWf/WKs0REttntI3dsjTHj8gur5PNBIB8IBbYDs/0dV6/4MoCF9u0YYD8wG/gR8P/8Hd8gMZcAyf3afgV81779XeCX/o5zgL+DSiAnkI4tcC6wENh1omMJXAn8BxBgGbA+AGK9FAi2b/+yV6y5vbcLoGM74O/e/p/bDoQBeXbOCPJnrP0e/y3wvyN9bMdzj9+7qLsxphvwLOoeEIwxFcaYLfbtFmAv1nrEY83VwKP27UeBa/wXyoAuAg4aY072ym+fMMa8D9T3ax7sWF4NPGYsHwLxIpIxKoEycKzGmDeNMU777odYK+kFhEGO7WCuBp4yxnQZY4qBQqzcMSqOF6uICPAJ4MmRft3xnPgHWtQ9IBOriOQCC4D1dtMd9kfohwNh6KQXA7wpIptFZIXdlmaMqbBvVwJp/gltUDfR9x8nUI8tDH4sA/1v+bNYn0g88kRkq4i8JyLn+CuoAQz0uw/kY3sOUGWMOdCrbUSO7XhO/GOCiEQDzwFfN8Y0A/cDU4D5QAXWR71AcbYxZiFwBfBlETm394PG+jwaMPOD7aU9PwY8YzcF8rHtI9CO5WBE5C7ACTxuN1UA2caYBcA3gSdEJNZf8fUyZn73vXySvp2WETu24znxB/yi7iISgpX0HzfGPA9gjKkyxriMMW7gr4zix84TMcaU29+rgRewYqvyDDvY36v9F+ExrgC2GGOqILCPrW2wYxmQf8sicitwFfBp+40Ke8ikzr69GWvMfLrfgrQd53cfqMc2GLgO+JenbSSP7XhO/AG9qLs9fvcQsNcY87te7b3Hbq8FdvXf1x9EJEpEYjy3sU7u7cI6prfYm90CvOifCAfUp8cUqMe2l8GO5UvAZ+zZPcuApl5DQn4hIpcD3wY+Zoxp79WeIiJB9u18YBpQ5J8ojzrO7/4l4CYRCRORPKx4N4x2fAO4GNhnjCnzNIzosR2ts9f++MKaDbEf653xLn/H0y+2s7E+yu8AttlfVwL/AHba7S8BGf6O1Y43H2v2w3Zgt+d4AknA28AB4C0g0d+x2nFFAXVAXK+2gDm2WG9IFUAP1rjy5wY7llizee6z/453AosDINZCrLFxz9/uX+xtP27/fWwDtgAfDZBjO+jvHrjLPrYFwBX+jtVufwT4Yr9tR+zYaskGpZSaYMbzUI9SSqkBaOJXSqkJRhO/UkpNMJr4lVJqgtHEr5RSE4wmfqV6EZGfiMjFI/A8rSe53xdE5DYRmS8iD5xqHEoNRKdzKuUDItJqjIk+if3+CfwQ64rYWmPM4yfYRalh0x6/GtdE5L9EZINdv/yBXlc+torIPWKthfC2iKTY7Y+IyPX27bvFWi9hh4j8xm7LFZF37La3RSTbbs8TkXVirVfws34x3CkiG+19fjxInN+w665fi1XG48fAXadUc12pQWjiV+OWiMwCbgTOMsbMB1zAp+2Ho4BNxpg5wHtYveze+yZhJeE5xpjTAU8y/yPwqN32OPAHu/1e4H5jzGlYV2J6nudSrEvrl2IVCFvUv7gdgDHmHuAS4B071v3GmNnGmC+eyjFQaiCa+NV4dhGwCNho96Yvwio9AeDmaAGsf2KV0OitCegEHhKR6wBPPZozgSfs2//otd9ZHK0L9I9ez3Op/bUV6zL7mVhvBANZCGy3Ky42DuUHVOpkBPs7AKV8SLB6598bwrZ9TnYZY5wishTrzeJ64A7gwuE8R68YfmGMGfRErYikAm8CqVhvNjcBMfab1ceNMQeHEL9SQ6Y9fjWevQ1cbydWz5q2OfZjDqyEDvAp4IPeO9rrJMQZY14DvgHMsx9ai5WYwRo2Wm3fXtOv3eMN4LP28yEimZ54PIwx1fbwzhasIaF/ArcZY+Zr0le+oIlfjVvGmD3A/2CtGrYDWIm11jFAG7BUrEWuLwR+0m/3GOAVe78PsBa+APgKcJvdfjPwNbv9a1iL0+yk1wpOxpg3sYaG1tmPPWs/dx/2SeckY0wtsJx+b0RKjSSdzqkmpJOdbqnUeKA9fqWUmmC0x6+UUhOM9viVUmqC0cSvlFITjCZ+pZSaYDTxK6XUBKOJXymlJpj/DwPyns9GduK+AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(len(scores)), scores)\n",
    "plt.ylabel('score')\n",
    "plt.xlabel('episode #')\n",
    "plt.savefig('ppo_training_history.png')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to see a trained agents, we can load the weights and run it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 36.09399919323623\n"
     ]
    }
   ],
   "source": [
    "trained_agent = Agent()\n",
    "trained_agent.load('ppo.pth')\n",
    "    \n",
    "score = evaluate_agent(trained_agent, fast=True)\n",
    "print(\"Score: {}\".format(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When finished, you can close the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlnd",
   "language": "python",
   "name": "drlnd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
